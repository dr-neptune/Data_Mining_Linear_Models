---
title: "Data Mining Regressions"
author: "Michael Rose"
output:
  html_document:
    code_folding: hide
    df_print: paged
    theme: paper
    hightlight: zenburn
    mathjax: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.align = "center", fig.width = 9)

data_location <- "~/Desktop/courses/Data_Mining/Admission_Predict.csv"

# programming
library(rstanarm)
core_num <- parallel::detectCores()
options(mc.cores = core_num)
library(tidyverse)
library(magrittr)
library(furrr)
plan(multiprocess)
library(mice)
library(projpred)

# graphing
library(corrplot)
library(cowplot)
library(GGally)

# color scheme
library(wesanderson)
color_scheme <- wes_palette("IsleofDogs1")

color_scheme <- c(color_scheme[1], color_scheme[2], color_scheme[3], color_scheme[4], color_scheme[5], color_scheme[6])
```

# {.tabset}

## Intro


#### Abstract

<center>

-----

This project is a data analysis culminating in the implementation of a series of Bayesian generalized linear models on data. The data is a series of observations related to masters program admissions in India. There are three separate models fit: Linear, Ordinal, and Beta regression.

-----

</center>


<center>

![](bayes_reg.png)

</center>

#### The Picture Above

In the picture above, we see the mathematical model of a simple linear regression. What makes this stand out as a Bayesian linear regression is that each of our parameters $\beta_n, \sigma$ all have their own prior distributions. Each of these prior distributions then has their own set of hyperparameters.



#### Bayesian vs. Frequentist Statistics

This analysis is unabashedly Bayesian. Consider Bayes' formula:

<center>

$P(A | B) = \frac{P(B | A)P(A)}{P(B)}$

</center>

We can reinterpret this as the following:

<center>

$P(\mathcal{H} | \mathcal{D}) = \frac{P(\mathcal{D} | \mathcal{H})P(\mathcal{H})}{P(\mathcal{D})}$

</center>

where $\mathcal{H}$ is a hypothesis and $\mathcal{D}$ is the data which gives evidence for or against $\mathcal{H}$.

In the equation above: 

- The prior $P(\mathcal{H})$ is the probability that our hypothesis is true before we look at our data

- The posterior $P(\mathcal{H} | \mathcal{D})$ is the probability that $\mathcal{H}$ is true after the data is considered

- The likelihood $P(\mathcal{D} | \mathcal{H})$ is the evidence about our hypothesis provided by the data

- $P(\mathcal{D})$ is the probability of the data taking into account all possible hypotheses

If we know our prior and likelihood, then we can compute our posterior exactly. This gives us a deductive logic of probability, and allows us to compare hypotheses, draw conclusions and make decisions. In most cases we do not know the prior probabilities for any given hypothesis. As recourse, we use statistical inference; We make up priors (Bayesian),or we rely on only our likelihood (Frequentist).

Bayesian inference models uncertainty by a probability distribution over hypotheses. Our ability to make inferences depends on our degree of confidence in our chosen prior. In frequentist statistics we assume that some hypothesis is true and that the observed data is sampled from that distribution. Particularly, frequentist statistics do not depend on subjective priors. 

For further contrast: 

|Bayesian Inference|Frequentist Inference|
|:--|:--|
|Probability for both hypothesis and data|No prior or posterior|
|Depends on the prior and likelihood of observed data|Depends on the likelihood for both observed and unobserved data|
|Requires constructing a subjective prior|No prior|
|May be computationally expensive|Generally less computationally intensive|

In the past there has been much discourse between Statisticians who tended to fall firmly in one or the other camp. These days, while there is still discourse, there tends to be much more of a focus on pragmatism as opposed to philosophical ideals as to how statistical analysis should be performed.



#### Methodology

Bayesian analysis consists of four steps: 

1. Specify a joint distribution for the outcome and all the unknowns. This takes the form of a marginal distribution for each unknown multiplied by the likelihood for the outcome conditional on the unknowns. 

2. Sample the posterior distribution using Markov Chain Monte Carlo techniques.

3. Evaluate the model fit on the data

4. Sample from the posterior predictive distribution of the outcome to get an idea of the values of the predictors. This allows us to understand how manipulating a predictor affects the outcome.


## Data 

### Overview of Data

The idea behind this dataset is to predict admissions into a Masters degree program. It was sampled from Engineering students at an Indian university. The parameters are the following: 

|  parameter|  range|  description|
|:--|:--|:--|
|  GRE Score|  0-340|  Score on GRE exam|
|  TOEFL Score|  0 - 120|  Score on TOEFL exam|
|  University Ranking|  0 / 5|  Indian University Ranking|
|  Statement of Purpose|  0 / 5|  Self assessed SOP score|
|  Letter of Reccommendation|  0  / 5|  Self assessed LOR score|
|  Undergraduate GPA|  0 / 10|  Cumulative undergraduate GPA|
|  Research Experience|  0 or 1|  1 if Student engaged in research, 0 otherwise|
|  Chance of Admit| $x \in [0, 1]$|  Likelihood of admission|

The source of this data is the following: 

-----

<center>

`A Comparison of Regression Models for Prediction of Graduate Admissions`

`Mohan S Acharya, Asfia Armaan, Aneeta S Antony`

`IEEE International Conference on Computational Intelligence in Data Science 2019`

</center>

-----

### Load Data

```{r}
# read data
admissions <- read_csv(data_location, 
                       # coerce data types
                       col_types = list(col_integer(), col_integer(), col_integer(), col_integer(), col_double(), col_double(), col_double(), col_factor(), col_double()))

# rename features
admissions %>% 
  rename("Student" = "Serial No.", 
         "GRE" = "GRE Score", 
         "TOEFL" = "TOEFL Score", 
         "Rating" = "University Rating",
         "GPA" = "CGPA",
         "Chance" = "Chance of Admit") -> admissions

admissions %>% head(50)
```

### Missing Values

```{r}
# check for missing values
admissions %>% md.pattern()
```

## Data Visualization

### Individual Features

```{r}
# grab colnames
admissions %>% select(-c("Student", "Research")) %>% colnames() -> adm_colnames

# make plotting function
plot_density <- function(variable){
  admissions %>%
    ggplot() + 
      geom_density(aes(x = !!sym(variable)), fill = color_scheme[1])
}

# get density plots for each variable
density_plots <- future_map(adm_colnames, ~plot_density(.x))

# make a special density plot for binary Research variable
admissions %>% 
  ggplot() + 
  geom_density(aes(x = Research, fill = Research), alpha = 0.5) + 
  scale_fill_manual(values = color_scheme) -> density_plots[[8]]

# plot 
density_plots %>%
    plot_grid(plotlist = ., ncol = 2)
```

### Combination of Predictors

```{r}
# make plotting function
plot_points <- function(data, mapping, ...){
  data %>%
    ggplot(mapping = mapping) + 
    geom_point(fill = color_scheme[1], color = "black", pch = 21) + 
    geom_smooth(method = "gam", color = color_scheme[4]) +
    scale_x_continuous(expand = expand_scale(mult = 0.3)) +
    scale_y_continuous(expand = expand_scale(mult = 0.3))
}

# grab lower plots from ggpairs
ggpairs_lower <- function(g){
  g$plots <- g$plots[-(1:g$nrow)]
  g$yAxisLabels <- g$yAxisLabels[-1]
  g$nrow <- g$nrow - 1
  g$plots <- g$plots[-(seq(g$ncol, length(g$plots), by = g$ncol))]
  g$xAxisLabels <- g$xAxisLabels[-g$ncol]
  g$ncol <- g$ncol - 1

  g
}

admissions %>% 
  select(-c("Student", "Research")) %>% 
  ggpairs(upper = NULL, diag = NULL, 
          lower = list(continuous = plot_points), progress = FALSE) %>% 
  ggpairs_lower()
```

### Correlations

```{r}
# create color palette for corrplot
col_ramped <- colorRampPalette(color_scheme)

# select features to plot
admissions %>% 
  select(-c("Student", "Research")) %>% 
  cor() %>% 
  corrplot(method = "shade")
```

We see that most of the predictor variables have relatively high correlation. 

## Summary Stats

|Statistic |GRE	|TOEFL	|Rating|	SOP|	LOR|	GPA|	Research|	Chance|
|:--|:--|:--|:--|:--|:--|:--|:--|:--|
|Min|290.0|92.0|1.000	|1.0	|1.000|6.800|1:219|0.3400|
|1st Qu.|308.0|103.0|2.000|2.5|3.000|8.170|0:181|0.6400|
|Median|317.0|107.0|3.000|3.5|3.500|8.610|NA|0.7300|
|Mean|316.8|107.4|3.087|3.4|3.453|8.599|NA|0.7244|
|3rd Qu.|325.0|112.0|4.000|4.0|4.000|9.062|NA|0.8300|
|Max|340.0|120.0|5.000|5.0|5.000|9.920|NA|0.9700|



## Linear

```{r, child = "lin_reg.Rmd"}

```


## Ordinal

```{r, child = "ord_reg.Rmd"}

```

## Beta

```{r, child = "beta_reg.Rmd"}

```


## Read More

Read More: 

Bayesian vs. Frequentist:

> https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading20.pdf

Data: 

> https://www.kaggle.com/mohansacharya/graduate-admissions

STAN: 

> http://mc-stan.org/rstanarm/index.html

> http://mc-stan.org/rstanarm/articles/continuous.html

> http://mc-stan.org/rstanarm/articles/polr.html

> http://mc-stan.org/rstanarm/articles/betareg.html

Regression: 

> https://en.wikipedia.org/wiki/Bayesian_linear_regression

> https://en.wikipedia.org/wiki/Ordinal_regression

> http://r-statistics.co/Beta-Regression-With-R.html